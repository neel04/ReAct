{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xPPOBJ1d2jL"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /fsx/awesome/DPT/\n",
    "# Changing directories to /fsx/awesome/DPT/ so that we can run the code from the DPT/deep-thinking/train_model.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOi3kGPCQvBo"
   },
   "source": [
    "## Writing the custom dataloader for `addition` task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RENTRY LINK: https://rentry.co/9966g, WHICH HAS MY VERSION FOR FFNET_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./deep-thinking/deepthinking/models/dt_net_1d.py\n",
    "\"\"\" dt_net_1d.py\n",
    "    DeepThinking 1D convolutional neural network.\n",
    "\n",
    "    Collaboratively developed\n",
    "    by Avi Schwarzschild, Eitan Borgnia,\n",
    "    Arpit Bansal, and Zeyad Emam.\n",
    "\n",
    "    Developed for DeepThinking project\n",
    "    October 2021\n",
    "\"\"\"\n",
    "import web_pdb as pdb\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from .blocks import BasicBlock1D as BasicBlock\n",
    "from .alibi import OptimizedALiBiMultiHeadAttention as ALiBiMHSA, VanillaALiBi\n",
    "from .rope import RoPE_MHA\n",
    "from .flash_mha import FlashMultiHeadAttention\n",
    "\n",
    "# Enabling SDP backend\n",
    "#torch.backends.cuda.enable_flash_sdp(enabled=True)\n",
    "#print(f'\\n{chr(0x26A1)*20}\\nFlash Attention status: {torch.backends.cuda.flash_sdp_enabled()}\\n{chr(0x26A1)*20}\\n')\n",
    "\n",
    "# Ignore statemenst for pylint:\n",
    "#     Too many branches (R0912), Too many statements (R0915), No member (E1101),\n",
    "#     Not callable (E1102), Invalid name (C0103), No exception (W0702)\n",
    "# pylint: disable=R0912, R0915, E1101, E1102, C0103, W0702, R0914\n",
    "class NewGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class AttentionBlock1D(nn.Module):\n",
    "    \"\"\"Basic MHSA residual block class for DeepThinking \"\"\"\n",
    "    \n",
    "    def __init__(self, drop_rate:int, width:int):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.activation = NewGELU()\n",
    "\n",
    "        self.attn_head = torch.nn.MultiheadAttention(self.width, self.width//32, bias=True, batch_first=True, dropout=0.05)\n",
    "        self.linear1 = nn.Linear(self.width, self.width)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(self.width)\n",
    "        self.ln2 = nn.LayerNorm(self.width)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width),\n",
    "            self.activation,\n",
    "            nn.Linear(self.width, self.width),\n",
    "            nn.Dropout(drop_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn_head(x, x, x, need_weights=False)[0]\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.mlp(x)\n",
    "\n",
    "        return self.activation(x)\n",
    "\n",
    "class DTNet1D(nn.Module):\n",
    "    \"\"\"DeepThinking 1D Network model class\"\"\"\n",
    "\n",
    "    def __init__(self, block, num_blocks, width, recall, group_norm=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.width = int(width) # width of the network layers\n",
    "        self.bottleneck = self.width // 2 # bottleneck width\n",
    "        self.recall = recall\n",
    "        self.SEQLEN = 16 # length of the input sequence\n",
    "        drop_rate = 0.1 # dropout rate\n",
    "\n",
    "        self.reshape_layer = nn.Linear(self.width, self.bottleneck) # downsampling layer\n",
    "        self.embed_layer = nn.Embedding(13, self.bottleneck, padding_idx=11) # embedding layer for the input sequence\n",
    "\n",
    "        proj_linear = nn.Linear(self.bottleneck, self.bottleneck)\n",
    "        head_linear = nn.Linear(self.bottleneck, 13)\n",
    "        \n",
    "        # Handling the recurrence \n",
    "        if self.recall:\n",
    "            recur_layers = [self.reshape_layer, NewGELU()]\n",
    "        else:\n",
    "            recur_layers = []\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            recur_layers.insert(0, AttentionBlock1D(drop_rate, width)) # add attention blocks to the beginning of the list\n",
    "\n",
    "        self.projection = nn.Sequential(proj_linear, NewGELU())\n",
    "        self.recur_block = nn.Sequential(*recur_layers)\n",
    "        self.head = nn.Sequential(head_linear, NewGELU())\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def positional_encoding(self, max_seq_len, d_model, device='cuda:0'):\n",
    "        '''\n",
    "        Generates the positional encoding for the input sequence\n",
    "        of shape (batch_size, max_seq_len, d_model) which would be added\n",
    "        to the sequence embeddings.\n",
    "        '''\n",
    "        pe = torch.zeros(max_seq_len, d_model, device=device)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        \n",
    "        return pe\n",
    "\n",
    "    def forward(self, x, iters_to_do, interim_thought=None, **kwargs):\n",
    "        # x -> (batch, 16)\n",
    "        x = self.embed_layer(x) + self.positional_encoding(self.SEQLEN, self.bottleneck, device=x.device)\n",
    "        initial_thought = self.projection(x)\n",
    "\n",
    "        if interim_thought is None:\n",
    "            interim_thought = initial_thought\n",
    "\n",
    "        # X -> (32, 1, 96), 32 is batch_size/#GPUs\n",
    "        all_outputs = torch.zeros((x.size(0), iters_to_do, self.SEQLEN, 13), device=x.device)\n",
    "\n",
    "        for i in range(iters_to_do):\n",
    "            if self.recall:\n",
    "                x = x.unsqueeze(-1) if x.dim() == 2 else x # (batch, 16) -> (batch, 16, 1) if needed\n",
    "                interim_thought = torch.cat([interim_thought, x], 2)\n",
    "\n",
    "            interim_thought = self.recur_block(interim_thought)\n",
    "            out = self.head(interim_thought)\n",
    "            all_outputs[:, i] = out\n",
    "\n",
    "        if self.training:\n",
    "            return out, interim_thought\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "def dt_net_1d(width, **kwargs):\n",
    "    return DTNet1D(BasicBlock, 1, width, recall=False)\n",
    "\n",
    "\n",
    "def dt_net_recall_1d(width, **kwargs):\n",
    "    return DTNet1D(BasicBlock, 1, width, recall=True)\n",
    "\n",
    "\n",
    "def dt_net_gn_1d(width, **kwargs):\n",
    "    return DTNet1D(BasicBlock, 1, width, recall=False, group_norm=True)\n",
    "\n",
    "\n",
    "def dt_net_recall_gn_1d(width, **kwargs):\n",
    "    return DTNet1D(BasicBlock, 1, width, recall=True, group_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tAdFfw0O963",
    "outputId": "3ea44e19-59a2-455a-bc08-32e1a5450d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./deep-thinking/deepthinking/utils/addition_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/deepthinking/utils/addition_data.py\n",
    "\"\"\"\n",
    "@author: neel04\n",
    "adapted from the Deep Thinking repo. New Version\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ArithmeticDataset(Dataset):\n",
    "  def __init__(self, mode, samples, seqlen, bits=None):\n",
    "    '''\n",
    "    Setting up the dataset to produce an arithmetic datast\n",
    "    with whatever operation\n",
    "    '''\n",
    "    assert mode in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    self.mode = mode\n",
    "    self.samples = samples\n",
    "    self.seqlen = seqlen\n",
    "    self.pad_token = 9\n",
    "\n",
    "    if self.mode == \"train\":\n",
    "        self.upper_b = 13\n",
    "        self.lower_b = 1\n",
    "    elif self.mode == \"val\":\n",
    "        self.upper_b = 14\n",
    "        self.lower_b = 13\n",
    "    else:\n",
    "        self.upper_b = 16\n",
    "        self.lower_b = 14\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.samples\n",
    "\n",
    "  def get_prefix_sum(self, x):\n",
    "    cum_sum = np.cumsum(x)\n",
    "    return [cum_sum[i] % 2 for i in range(len(cum_sum))]\n",
    "\n",
    "  def pad_sequence(self, seq, max_len):\n",
    "    return seq[:max_len] + [self.pad_token] * (max_len - len(seq))\n",
    "  \n",
    "  def decode(self, x):\n",
    "      return \"\".join(str(i) for i in x if i != self.pad_token)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    seq_len = random.randint(self.lower_b, self.upper_b) # random length of the sequence\n",
    "    seq = [random.randint(0, 1) for i in range(seq_len)]\n",
    "\n",
    "    prefix_sum = self.get_prefix_sum(seq)\n",
    "    padded_seq = self.pad_sequence(seq, self.seqlen)\n",
    "    padded_prefix_sum = self.pad_sequence(prefix_sum, self.seqlen)\n",
    "\n",
    "    return torch.tensor(padded_seq), torch.tensor(padded_prefix_sum)\n",
    "\n",
    "\n",
    "def prepare_addition_loader(train_batch_size, test_batch_size, train_data, test_data, shuffle=False):\n",
    "    # We ignore the train_data and test_data rather than removing for compatibility reasons\n",
    "    \n",
    "    train_dataset = ArithmeticDataset(mode='train', samples=50_000, seqlen=16, bits=6)\n",
    "    val_dataset = ArithmeticDataset(mode='val', samples=50_000, seqlen=16, bits=6)\n",
    "    test_dataset = ArithmeticDataset(mode='test', samples=5_000, seqlen=16, bits=6)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset,\n",
    "                             num_workers=2,\n",
    "                             batch_size=train_batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=True,\n",
    "                             pin_memory=True,\n",
    "                             prefetch_factor=8)\n",
    "\n",
    "    valloader = DataLoader(val_dataset,\n",
    "                             num_workers=2,\n",
    "                             batch_size=test_batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             pin_memory=True,\n",
    "                             persistent_workers=True,\n",
    "                             prefetch_factor=2)\n",
    "\n",
    "    testloader = DataLoader(test_dataset,\n",
    "                             num_workers=2,\n",
    "                             batch_size=test_batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             pin_memory=True,\n",
    "                             persistent_workers=True,\n",
    "                             prefetch_factor=4)\n",
    "\n",
    "    loaders = {\"train\": trainloader, \"test\": testloader, \"val\": valloader}\n",
    "\n",
    "    return loaders\n",
    "    print(f'\\nAddition dataloaders have been succesfully created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6UqWoU0bVd-"
   },
   "source": [
    "## Setting up correct imports, inserting dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saweb0dfbaFv",
    "outputId": "4219a9ea-58c6-4284-dbb3-78c2a1779e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/deep-thinking/deepthinking/utils/tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/deepthinking/utils/tools.py\n",
    "\"\"\" tools.py\n",
    "    Utility functions that are common to all tasks\n",
    "\n",
    "    Collaboratively developed\n",
    "    by Avi Schwarzschild, Eitan Borgnia,\n",
    "    Arpit Bansal, and Zeyad Emam.\n",
    "\n",
    "    Developed for DeepThinking project\n",
    "    October 2021\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from icecream import ic\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "from .lion_opt import Lion, AdamOnLion \n",
    "\n",
    "import deepthinking.models as models\n",
    "from .mazes_data import prepare_maze_loader\n",
    "from .prefix_sums_data import prepare_prefix_loader\n",
    "from .chess_data import prepare_chess_loader #ADDED NEW\n",
    "from .addition_data import prepare_addition_loader\n",
    "from .. import adjectives, names\n",
    "\n",
    "from .warmup import ExponentialWarmup, LinearWarmup\n",
    "\n",
    "# Ignore statements for pylint:\n",
    "#     Too many branches (R0912), Too many statements (R0915), No member (E1101),\n",
    "#     Not callable (E1102), Invalid name (C0103), No exception (W0702),\n",
    "#     Too many local variables (R0914), Missing docstring (C0116, C0115).\n",
    "# pylint: disable=R0912, R0915, E1101, E1102, C0103, W0702, R0914, C0116, C0115\n",
    "\n",
    "\n",
    "def generate_run_id():\n",
    "    hashstr = f\"{adjectives[random.randint(0, len(adjectives))]}-{names[random.randint(0, len(names))]}\"\n",
    "    return hashstr\n",
    "\n",
    "\n",
    "def get_dataloaders(problem_args):\n",
    "    if problem_args.name == \"prefix_sums\":\n",
    "        return prepare_prefix_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                     test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                     train_data=problem_args.train_data,\n",
    "                                     test_data=problem_args.test_data)\n",
    "    elif problem_args.name == \"mazes\":\n",
    "        return prepare_maze_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                   test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                   train_data=problem_args.train_data,\n",
    "                                   test_data=problem_args.test_data)\n",
    "    elif problem_args.name == \"chess\":\n",
    "        return prepare_chess_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                    test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                    train_data=problem_args.train_data,\n",
    "                                    test_data=problem_args.test_data)\n",
    "    elif problem_args.name == \"addition\":\n",
    "        return prepare_addition_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                    test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                    train_data=problem_args.train_data,\n",
    "                                    test_data=problem_args.test_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid problem spec. {problem_args.name}\")\n",
    "\n",
    "\n",
    "def get_model(model, width, max_iters, in_channels=3):\n",
    "    model = model.lower()\n",
    "    net = getattr(models, model)(width=width, in_channels=in_channels, max_iters=max_iters)\n",
    "    print(net,'\\n\\n')\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_optimizer(optim_args, model_args, net, state_dict):\n",
    "    optimizer_name = optim_args.optimizer.lower()\n",
    "    epochs = optim_args.epochs\n",
    "    lr = optim_args.lr\n",
    "    lr_decay = optim_args.lr_decay\n",
    "    lr_schedule = optim_args.lr_schedule\n",
    "    lr_factor = optim_args.lr_factor\n",
    "    warmup_period = optim_args.warmup_period\n",
    "\n",
    "    if optim_args.lr_throttle:\n",
    "        # Reducing the lr here for the recurrent layers helps with stability,\n",
    "        # To date (July 21, 2021), we may only need this for maze models.\n",
    "        base_params = [p for n, p in net.named_parameters() if \"recur\" not in n]\n",
    "        recur_params = [p for n, p in net.named_parameters() if \"recur\" in n]\n",
    "        iters = model_args.max_iters\n",
    "        all_params = [{\"params\": base_params}, {\"params\": recur_params, \"lr\": lr / iters}]\n",
    "    else:\n",
    "        base_params = [p for n, p in net.named_parameters()]\n",
    "        recur_params = []\n",
    "        iters = 1\n",
    "        all_params = [{\"params\": base_params}]\n",
    "\n",
    "    if optimizer_name == \"sgd\":\n",
    "        optimizer = SGD(all_params, lr=lr, weight_decay=2e-3, momentum=0.9)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        optimizer = Adam(all_params, lr=lr, weight_decay=2e-3)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = AdamW(all_params, lr=lr, weight_decay=2e-3)\n",
    "    elif optimizer_name == \"lion\":\n",
    "        optimizer = Lion(all_params, lr=lr, weight_decay=7e-3, betas=(0.9, 0.99))\n",
    "    elif optimizer_name == \"adam_on_lion\":\n",
    "        optimizer = AdamOnLion(all_params, lr=lr, weight_decay=2e-3, betas=(0.9, 0.99))\n",
    "    else:\n",
    "        raise ValueError(f\"{ic.format()}: Optimizer choise of {optimizer_name} not yet implmented.\")\n",
    "\n",
    "    if state_dict is not None:\n",
    "        optimizer.load_state_dict(state_dict)\n",
    "        optimizer.param_groups[0][\"capturable\"] = True # make optimizer capturable=True\n",
    "        warmup_scheduler = ExponentialWarmup(optimizer, warmup_period=0)\n",
    "        # warmup_scheduler = LinearWarmup(optimizer, warmup_period=0)\n",
    "    else:\n",
    "        warmup_scheduler = ExponentialWarmup(optimizer, warmup_period=warmup_period)\n",
    "        # warmup_scheduler = LinearWarmup(optimizer, warmup_period=warmup_period)\n",
    "\n",
    "    if lr_decay.lower() == \"step\":\n",
    "        lr_scheduler = MultiStepLR(optimizer, milestones=lr_schedule,\n",
    "                                   gamma=lr_factor, last_epoch=-1)\n",
    "    elif lr_decay.lower() == \"cosine\":\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, epochs, eta_min=0, last_epoch=-1, verbose=False)\n",
    "    else:\n",
    "        raise ValueError(f\"{ic.format()}: Learning rate decay style {lr_decay} not yet implemented.\")\n",
    "\n",
    "    return optimizer, warmup_scheduler, lr_scheduler\n",
    "\n",
    "\n",
    "def load_model_from_checkpoint(problem, model_args, device, accelerator):\n",
    "    model = model_args.model\n",
    "    model_path = model_args.model_path\n",
    "    width = model_args.width\n",
    "    max_iters = model_args.max_iters\n",
    "    epoch = 0\n",
    "    optimizer = None\n",
    "    new_state_dict = {}\n",
    "\n",
    "    in_channels = 3\n",
    "    if problem == \"chess\":\n",
    "        in_channels = 12\n",
    "    elif problem == 'addition':\n",
    "        in_channels = 1\n",
    "\n",
    "    net = get_model(model, width, in_channels=in_channels, max_iters=max_iters)\n",
    "    net = net.to(device)\n",
    "    if device == \"cuda\":\n",
    "        net = net\n",
    "    \n",
    "    if model_path is not None and os.path.exists(model_path):\n",
    "        logging.info(f\"\\n{'$'*50}\\nLoading model from checkpoint {model_path}...\\n{'$'*50}\")\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "        # check if keys are prefixed with \"module.\"\n",
    "        new_state_dict = state_dict.copy()\n",
    "\n",
    "        for key in list(new_state_dict[\"net\"].keys()):\n",
    "            new_key = key.replace('_orig_mod.', '') # remove _orig_mod. prefix\n",
    "            new_state_dict[\"net\"][new_key] = state_dict['net'][key]\n",
    "            # remove old key\n",
    "            del new_state_dict[\"net\"][key]\n",
    "        \n",
    "        # Now load fixed state_dict\n",
    "        net.load_state_dict(new_state_dict[\"net\"])\n",
    "        epoch = new_state_dict[\"epoch\"] + 1\n",
    "        optimizer = new_state_dict[\"optimizer\"]\n",
    "        accelerator.load_state(f\"/fsx/DPT/outputs/{model_path}\")\n",
    "\n",
    "    return net, epoch, optimizer, accelerator\n",
    "\n",
    "\n",
    "def now():\n",
    "    return datetime.now().strftime(\"%Y%m%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddtuzMJpxPVj"
   },
   "source": [
    "## Config `YAML`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6sYoiHJgHW_"
   },
   "source": [
    "Hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVnF0eHpgCVn",
    "outputId": "e5b24aab-eb40-44f2-970b-28a572059f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/deep-thinking/config/problem/hyp/addition_default.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/config/problem/hyp/addition_default.yaml\n",
    "alpha: 1\n",
    "clip: 2\n",
    "epochs: 175\n",
    "lr: 9e-5 # 5e-4 does get to 98% elemennwise accuracy in 100 epochs\n",
    "lr_decay: cosine\n",
    "lr_factor: 0.1\n",
    "lr_schedule: #  CosineAnnealingLR Doesn't use this param\n",
    "  - 2\n",
    "lr_throttle: False\n",
    "optimizer: lion\n",
    "save_period: -1\n",
    "test_batch_size: 768\n",
    "test_mode: default\n",
    "train_batch_size: 768\n",
    "train_mode: progressive\n",
    "val_period: 20\n",
    "warmup_period: 8 # We use Cosine warmup for the first 5 epochs, low warmup for DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Look about using difference loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUpaWeQFaEry",
    "outputId": "55bef61e-c269-4777-f3b9-33d8c25d0b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/deep-thinking/config/problem/addition.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/config/problem/addition.yaml\n",
    "defaults:\n",
    "  - hyp: addition_default\n",
    "  - model: dt_net_recall_1d\n",
    "\n",
    "name: addition\n",
    "test_data: 5_000\n",
    "train_data: 50_000\n",
    "\n",
    "model:\n",
    "  model_path:\n",
    "  width: 256\n",
    "  max_iters: 15\n",
    "  test_iterations:\n",
    "    low: 15\n",
    "    high: 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwgxRicdd7cp"
   },
   "source": [
    "## Executing the training for Arithmetic 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10SHnwP5mn3i"
   },
   "source": [
    "Shape -> `(24, 20)`\n",
    "- w denotes the width of the model\n",
    "- Prefix sum used width = 400, so bump 128 → 400\n",
    "- iterations can be pushed to [50,100]\n",
    "- Epochs → 100\n",
    "- Grid search for α ∈ [0,1]\n",
    "- [x]  Try L1/L2 loss\n",
    "\n",
    "`detnet_1d.py` has been modified\n",
    "`training.py` has been modified\n",
    "`testing.py` has been modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHZTnqLqc7fM",
    "outputId": "774e43f3-c6be-439d-f910-821a178e08ba"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import torch\n",
    "import os\n",
    "\n",
    "%env HYDRA_FULL_ERROR=1\n",
    "%cd /fsx/awesome/DPT/deep-thinking\n",
    "\n",
    "# generate random port b/w 20000-30000\n",
    "port = randrange(20_000, 30_000)\n",
    "rdvz_id = randrange(100, 999)\n",
    "#!python3 train_model.py problem=addition name=addition_run\n",
    "# we have to launch above script with torchrun on a single host, with 8 GPUs\n",
    "%env OMP_NUM_THREADS=2\n",
    "%env PORT=$port\n",
    "%env RDVZ_ID=$rdvz_id\n",
    "#!torchrun --nproc_per_node=1 --nnodes=1 --rdzv_id=$RDVZ_ID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:$PORT train_model.py problem=addition name=addition_run\n",
    "# Using Huggigface accelerate with DDP and 1 GPU\n",
    "#!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 train_model.py problem=addition name=addition_run\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {n_gpus}\")\n",
    "tpu_available = 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "%env NUM_GPUS=$n_gpus\n",
    "\n",
    "if n_gpus > 0:\n",
    "    !accelerate launch --config_file /fsx/awesome/DPT/configs/acc_config.yaml --num_processes=$NUM_GPUS train_model.py problem=addition name=addition_run\n",
    "elif tpu_available:\n",
    "    print(\"TPU is available\")\n",
    "    !accelerate launch --config_file /fsx/awesome/DPT/configs/acc_tpu_config.yaml train_model.py problem=addition name=addition_run\n",
    "else:\n",
    "    print(\"No GPU or TPU available\")\n",
    "    !accelerate launch --config_file /fsx/awesome/DPT/configs/acc_cpu_config.yaml train_model.py problem=addition name=addition_run\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EV8ezMeXxe8i",
    "kOi3kGPCQvBo",
    "h6UqWoU0bVd-",
    "ddtuzMJpxPVj"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b81162e66a07f2e6ff8b866a7ffebbba4c947ba8c7b07af7b7620290636d340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
