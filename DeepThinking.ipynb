{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9xPPOBJ1d2jL",
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /fsx/awesome/DPT/\n",
    "# Changing directories to /fsx/awesome/DPT/ so that we can run the code from the DPT/deep-thinking/train_model.py file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kOi3kGPCQvBo"
   },
   "source": [
    "## Writing the custom dataloader for `addition` task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RENTRY LINK: https://rentry.co/9966g, WHICH HAS MY VERSION FOR FFNET_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./deep-thinking/deepthinking/models/dt_net_1d.py\n",
    "\"\"\" dt_net_1d.py\n",
    "    DeepThinking 1D convolutional neural network.\n",
    "\n",
    "    Collaboratively developed\n",
    "    by Avi Schwarzschild, Eitan Borgnia,\n",
    "    Arpit Bansal, and Zeyad Emam.\n",
    "\n",
    "    Developed for DeepThinking project\n",
    "    October 2021\n",
    "\"\"\"\n",
    "import web_pdb as pdb\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "#from .blocks import BasicBlock1D as BasicBlock\n",
    "#from .alibi import OptimizedALiBiMultiHeadAttention as ALiBiMHSA, VanillaALiBi\n",
    "#from .rope import RoPE_MHA\n",
    "#from .flash_mha import FlashMultiHeadAttention\n",
    "\n",
    "# Ignore statemenst for pylint:\n",
    "#     Too many branches (R0912), Too many statements (R0915), No member (E1101),\n",
    "#     Not callable (E1102), Invalid name (C0103), No exception (W0702)\n",
    "# pylint: disable=R0912, R0915, E1101, E1102, C0103, W0702, R0914\n",
    "class NewGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "def next_power(x: int):\n",
    "    i = 1\n",
    "    while i < x: i *= 2 # power of 2 less than x\n",
    "    return i\n",
    "\n",
    "class LiteAttention(nn.Module):\n",
    "    '''\n",
    "    Compute a data dependent vector of attention weights\n",
    "    which is hadamard multiplied with the actual input to produce a weighted output. \n",
    "    Use Softmax to normalize the attention weights.\n",
    "    '''\n",
    "    def __init__(self, dim: int, s_dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.attention_weights = nn.Linear(dim, dim, bias=False)\n",
    "        self.gate = nn.Softmax(dim=s_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.gate(self.attention_weights(x))\n",
    "        return x * attention_weights\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Basic MHSA residual block class for DeepThinking \"\"\"\n",
    "    \n",
    "    def __init__(self, drop_rate:float, bottleneck: int):\n",
    "        super().__init__()\n",
    "        self.activation = NewGELU()\n",
    "        self.input_dim = bottleneck * 2\n",
    "\n",
    "        self.attn_gate = LiteAttention(self.input_dim)\n",
    "        #self.attn_gate = torch.nn.MultiheadAttention(self.input_dim, next_power(self.width // 64), bias=True, dropout=drop_rate)\n",
    "        self.ln1 = nn.LayerNorm(self.input_dim)\n",
    "        self.ln2 = nn.LayerNorm(self.input_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.input_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.input_dim, self.input_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(self.input_dim, self.input_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(drop_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attn_gate(x)\n",
    "        #x = x + self.attn_gate(x,x,x, need_weights=False)[0]\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RecurrentModule(nn.Module):\n",
    "    \"\"\"Main Module for holding recurrent modules (w/ skip connections)\"\"\"\n",
    "\n",
    "    def __init__(self, num_blocks: int, drop_rate: float, bottleneck: int, recall: bool = True):\n",
    "        super(RecurrentModule, self).__init__()\n",
    "        self.recall: bool = recall\n",
    "\n",
    "        # Define the layers\n",
    "        self.gelu = NewGELU()\n",
    "        self.reshape_layer = nn.Linear(bottleneck * 2, bottleneck) # downsampling layer\n",
    "        self.ln = nn.LayerNorm(bottleneck * 2)\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            AttentionBlock(drop_rate, bottleneck)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Add attention blocks with skip connections\n",
    "        for attention_block in self.attention_blocks:\n",
    "            x = x + attention_block(x)\n",
    "        \n",
    "        x = self.ln(x) # Norm to provide stability\n",
    "        # Handling the recurrence by downsampling\n",
    "        if self.recall:\n",
    "            x = self.gelu(self.reshape_layer(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class OutputModule(nn.Module):\n",
    "    def __init__(self, bottleneck: int, num_predictions: int, SEQLEN: int, tgt_vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(bottleneck, num_predictions),\n",
    "            NewGELU()\n",
    "        )\n",
    "        self.head_2 = nn.Sequential(\n",
    "            nn.Linear(SEQLEN, tgt_vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.head(x) # (batch_size, SEQLEN, num_predictions)\n",
    "        x = x.transpose(-1, -2) # (batch_size, num_predictions, SEQLEN)\n",
    "        x = self.head_2(x) # (batch_size, num_predictions, tgt_vocab_size)\n",
    "        x = x.transpose(-1, -2) # (batch_size, tgt_vocab_size, num_predictions)\n",
    "        return x\n",
    "\n",
    "class DTNet1D(nn.Module):\n",
    "    \"\"\"DeepThinking 1D Network model class\"\"\"\n",
    "\n",
    "    def __init__(self, num_blocks, width, recall, group_norm=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.recall: bool = recall\n",
    "        self.width = int(width) # width of the network layers\n",
    "        self.bottleneck = width // 2 # bottleneck width\n",
    "\n",
    "        self.num_predictions = 1 # number of predictions to make | 1 -> MLM, >1 -> Seq2Seq/span prediction\n",
    "        self.vocab_size = 16\n",
    "        self.tgt_vocab_size = 2 # usually equal to vocab_size\n",
    "        self.embed_d = 128\n",
    "        self.SEQLEN = 32 # length of the input sequence\n",
    "        self.drop_rate = 0.15 # dropout rate\n",
    "\n",
    "        self.embed_layer = nn.Embedding(self.vocab_size, self.embed_d, padding_idx=10) # embedding layer for the input sequence\n",
    "        self.recur_block = RecurrentModule(num_blocks, self.drop_rate, self.bottleneck, self.recall) # Main recurrent block\n",
    "        self.reshape_head = nn.Linear(self.embed_d, self.bottleneck)\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bottleneck, self.bottleneck),\n",
    "            NewGELU()\n",
    "        )\n",
    "        \n",
    "        self.out_head = OutputModule(self.bottleneck, self.num_predictions, self.SEQLEN, self.tgt_vocab_size) # Output module\n",
    "        # Cache positional encoding\n",
    "        self.positional_encodings = self.positional_encoding(self.embed_d).to(torch.device('cuda:0'))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def positional_encoding(self, d_model):\n",
    "        '''\n",
    "        Generates the positional encoding for the input sequence\n",
    "        of shape (batch_size, max_seq_len, d_model) which would be added\n",
    "        to the sequence embeddings.\n",
    "        '''\n",
    "        position = torch.arange(self.SEQLEN, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(self.SEQLEN, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x, iters_to_do, interim_thought=None, **kwargs):\n",
    "        # x -> (batch, SEQLEN)\n",
    "        x = self.embed_layer(x) + self.positional_encodings # (batch, SEQLEN, embed_d)\n",
    "        x = self.reshape_head(x) # (batch, SEQLEN, bottleneck)\n",
    "\n",
    "        initial_thought = self.projection(x) # The first branch of the network\n",
    "        \n",
    "        if interim_thought is None:\n",
    "            interim_thought = initial_thought\n",
    "\n",
    "        # x -> (32, *), 32 is batch_size/#GPUs\n",
    "        all_outputs = torch.zeros((x.size(0), iters_to_do, self.tgt_vocab_size, self.num_predictions), device=x.device) if not self.training else None\n",
    "\n",
    "        # Funneling all branches into a single, recursive set of blocks repeated `iters_to_do` times\n",
    "        for i in range(iters_to_do):\n",
    "            if self.recall:\n",
    "                x = x.unsqueeze(-1) if x.dim() == 2 else x # (batch, 16) -> (batch, 16, 1) if needed otherwise no-op\n",
    "                # interim_thought is shape: (batch, SEQLEN, bottleneck) | x is shape: (batch, SEQLEN, bottleneck)\n",
    "                interim_thought = torch.cat([interim_thought, x], 2) # (batch, SEQLEN, 2*bottleneck)\n",
    "\n",
    "            interim_thought = self.recur_block(interim_thought) # the recursive block, bulk of the network | (batch, SEQLEN, bottleneck)\n",
    "\n",
    "            if not self.training:\n",
    "                # During testing, we need out for every iteration to append to all_outputs\n",
    "                out = self.out_head(interim_thought) # (batch, tgt_vocab_size, num_predictions)\n",
    "                all_outputs[:, i] = out # storing intermediate computations for each iteration\n",
    "\n",
    "        if self.training:\n",
    "            # During training, we only need output when all the iterations are done, saving compute\n",
    "            out = self.out_head(interim_thought) # (batch, tgt_vocab_size, num_predictions)\n",
    "            return out, interim_thought\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "def dt_net_1d(width, **kwargs):\n",
    "    return DTNet1D(3, width, recall=False)\n",
    "\n",
    "\n",
    "def dt_net_recall_1d(width, **kwargs):\n",
    "    return DTNet1D(3, width, recall=True)\n",
    "\n",
    "\n",
    "def dt_net_gn_1d(width, **kwargs):\n",
    "    return DTNet1D(3, width, recall=False, group_norm=True)\n",
    "\n",
    "\n",
    "def dt_net_recall_gn_1d(width, **kwargs):\n",
    "    return DTNet1D(3, width, recall=True, group_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tAdFfw0O963",
    "outputId": "3ea44e19-59a2-455a-bc08-32e1a5450d13"
   },
   "outputs": [],
   "source": [
    "%%writefile ./deep-thinking/deepthinking/utils/addition_data.py\n",
    "\"\"\"\n",
    "@author: neel04\n",
    "adapted from the Deep Thinking repo. New Version\n",
    "\"\"\"\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PrefixDataset(Dataset):\n",
    "  '''\n",
    "  wrapper around prefix sum data gen function\n",
    "  '''\n",
    "  def __init__(self, mode, samples, seqlen):\n",
    "    assert mode in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    self.mode = mode\n",
    "    self.samples = samples\n",
    "    self.seqlen = seqlen\n",
    "    self.tokens = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' <PAD>', '+', '=', '_', '[MASK]']\n",
    "\n",
    "    self.tok_dict = {token: index for index, token in enumerate(self.tokens)}\n",
    "    self.inv_tok_dict = {index: token for index, token in enumerate(self.tokens)}\n",
    "\n",
    "    self.pad_token = self.tok_dict[' <PAD>'] # has to be > len(self.tokens)\n",
    "    self.masking_token = self.pad_token #self.tok_dict['[MASK]']\n",
    "\n",
    "    self.tokenizer = lambda x: [self.tok_dict[token] for token in x]\n",
    "\n",
    "    if self.mode == \"train\": # bounds for the length of digits in EACH number @ different stages\n",
    "        self.upper_b = 9\n",
    "        self.lower_b = 1\n",
    "    elif self.mode == \"val\":\n",
    "        self.upper_b = 12\n",
    "        self.lower_b = 9\n",
    "    else:\n",
    "        self.upper_b = 15\n",
    "        self.lower_b = 12\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.samples\n",
    "  \n",
    "  def get_prefix_sum(self, x: list):\n",
    "    cum_sum = np.cumsum(x)\n",
    "    return [cum_sum[i] % 2 for i in range(len(cum_sum))]\n",
    "\n",
    "  def stringify(self, x: list):\n",
    "    return ''.join(str(i) for i in x)\n",
    "\n",
    "  def generate_sequence(self, n_digits: int) -> tuple:\n",
    "      base_seq = [random.randint(0, 1) for _ in range(n_digits)]\n",
    "      prefix_sum = self.get_prefix_sum(base_seq)\n",
    "      base_seq, prefix_sum = self.stringify(base_seq), self.stringify(prefix_sum)\n",
    "\n",
    "      sequence = f'{base_seq}={prefix_sum}'\n",
    "      tokenized_seq = self.tokenizer(sequence)\n",
    "\n",
    "      return tokenized_seq\n",
    "\n",
    "  def pad_sequence(self, seq: torch.Tensor, max_len: int, rndm_num:int):\n",
    "    if self.mode == \"train\":\n",
    "        # pads the sequence with random number of pad tokens on both sides\n",
    "        return [self.pad_token] * rndm_num + seq + [self.pad_token] * (max_len - len(seq) - rndm_num)\n",
    "    else:\n",
    "        # this is the default padding for val and test\n",
    "        return seq[:max_len] + [self.pad_token] * (max_len - len(seq))\n",
    "\n",
    "  def decode(self, x: torch.Tensor):\n",
    "    x = x.view(-1).tolist()\n",
    "\n",
    "    #if len(set(x)) >= 3: # check to see if its inputs, because outputs is binary in [0, 1]\n",
    "    return ''.join(self.inv_tok_dict[int(elem)] if elem in self.tok_dict.values() else '' for elem in x).strip()\n",
    "\n",
    "  def corrupt_sequence(self, sequence: torch.Tensor):\n",
    "    # Find indices of non-padding elements\n",
    "    mask = (sequence != self.pad_token) & (sequence != self.tok_dict['=']) & (sequence != self.tok_dict['+'])\n",
    "    non_padding_indices = torch.nonzero(mask, as_tuple=True)[0]\n",
    "    assert len(non_padding_indices) != 0, f'No non-padding elements found in the sequence:\\n{sequence}'\n",
    "\n",
    "    # Choose random index from non-padding indices\n",
    "    random_index = torch.randint(len(non_padding_indices), size=(1,))\n",
    "    chosen_index = non_padding_indices[random_index]\n",
    "\n",
    "    # Get the token at the chosen index\n",
    "    replaced_token = sequence[chosen_index].item()\n",
    "\n",
    "    # Create a new sequence by replacing the chosen token with the masking token\n",
    "    corrupted_sequence = sequence.clone()\n",
    "    corrupted_sequence[chosen_index] = self.masking_token\n",
    "\n",
    "    return corrupted_sequence, replaced_token\n",
    "\n",
    "  def __getitem__(self, idx: int):\n",
    "    n_digits = np.random.randint(self.lower_b, self.upper_b + 1)\n",
    "    src_seq = self.generate_sequence(n_digits)\n",
    "\n",
    "    maxlen = len(src_seq)\n",
    "    rndm_num = random.randint(0, self.seqlen - maxlen) if (self.seqlen - maxlen) >= 0 else 0  # set it to 0 to prevent error\n",
    "\n",
    "    padded_src_seq = torch.Tensor(self.pad_sequence(src_seq, self.seqlen, rndm_num)).long()\n",
    "\n",
    "    corrupted_seq, mask_label = self.corrupt_sequence(padded_src_seq)\n",
    "    label_seq = torch.tensor(mask_label)\n",
    "\n",
    "    return corrupted_seq, label_seq # corrupt_seq: (1, seqlen), label_seq: (1)\n",
    "\n",
    "def prepare_addition_loader(train_batch_size, test_batch_size, train_data, test_data, shuffle=False):\n",
    "    # We ignore the train_data and test_data rather than removing for compatibility reasons\n",
    "    \n",
    "    train_dataset = PrefixDataset(mode='train', samples=50_000, seqlen=32)\n",
    "    val_dataset = PrefixDataset(mode='val', samples=50_000, seqlen=32)\n",
    "    test_dataset = PrefixDataset(mode='test', samples=5_000, seqlen=32)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset,\n",
    "                             num_workers=8,\n",
    "                             batch_size=train_batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             drop_last=True,\n",
    "                             pin_memory=True,\n",
    "                             prefetch_factor=32)\n",
    "\n",
    "    valloader = DataLoader(val_dataset,\n",
    "                             num_workers=8,\n",
    "                             batch_size=test_batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             pin_memory=True,\n",
    "                             persistent_workers=True,\n",
    "                             prefetch_factor=64)\n",
    "\n",
    "    testloader = DataLoader(test_dataset,\n",
    "                             num_workers=8,\n",
    "                             batch_size=test_batch_size,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             pin_memory=True,\n",
    "                             persistent_workers=True,\n",
    "                             prefetch_factor=64)\n",
    "\n",
    "    loaders = {\"train\": trainloader, \"test\": testloader, \"val\": valloader}\n",
    "\n",
    "    return loaders\n",
    "    print(f'\\nAddition dataloaders have been succesfully created!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h6UqWoU0bVd-"
   },
   "source": [
    "## Setting up correct imports, inserting dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saweb0dfbaFv",
    "outputId": "4219a9ea-58c6-4284-dbb3-78c2a1779e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/deep-thinking/deepthinking/utils/tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/deepthinking/utils/tools.py\n",
    "\"\"\" tools.py\n",
    "    Utility functions that are common to all tasks\n",
    "\n",
    "    Collaboratively developed\n",
    "    by Avi Schwarzschild, Eitan Borgnia,\n",
    "    Arpit Bansal, and Zeyad Emam.\n",
    "\n",
    "    Developed for DeepThinking project\n",
    "    October 2021\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from icecream import ic\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "from .lion_opt import Lion, AdamOnLion \n",
    "\n",
    "import deepthinking.models as models\n",
    "from .mazes_data import prepare_maze_loader\n",
    "from .prefix_sums_data import prepare_prefix_loader\n",
    "from .chess_data import prepare_chess_loader #ADDED NEW\n",
    "from .addition_data import prepare_addition_loader\n",
    "from .. import adjectives, names\n",
    "\n",
    "from .warmup import ExponentialWarmup, LinearWarmup\n",
    "\n",
    "# Ignore statements for pylint:\n",
    "#     Too many branches (R0912), Too many statements (R0915), No member (E1101),\n",
    "#     Not callable (E1102), Invalid name (C0103), No exception (W0702),\n",
    "#     Too many local variables (R0914), Missing docstring (C0116, C0115).\n",
    "# pylint: disable=R0912, R0915, E1101, E1102, C0103, W0702, R0914, C0116, C0115\n",
    "\n",
    "\n",
    "def generate_run_id():\n",
    "    hashstr = f\"{adjectives[random.randint(0, len(adjectives))]}-{names[random.randint(0, len(names))]}\"\n",
    "    return hashstr\n",
    "\n",
    "\n",
    "def get_dataloaders(problem_args):\n",
    "    if problem_args.name == \"prefix_sums\":\n",
    "        return prepare_prefix_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                     test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                     train_data=problem_args.train_data,\n",
    "                                     test_data=problem_args.test_data)\n",
    "    elif problem_args.name == \"mazes\":\n",
    "        return prepare_maze_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                   test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                   train_data=problem_args.train_data,\n",
    "                                   test_data=problem_args.test_data)\n",
    "    elif problem_args.name == \"chess\":\n",
    "        return prepare_chess_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                    test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                    train_data=problem_args.train_data,\n",
    "                                    test_data=problem_args.test_data)\n",
    "    elif problem_args.name == \"addition\":\n",
    "        return prepare_addition_loader(train_batch_size=problem_args.hyp.train_batch_size,\n",
    "                                    test_batch_size=problem_args.hyp.test_batch_size,\n",
    "                                    train_data=problem_args.train_data,\n",
    "                                    test_data=problem_args.test_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid problem spec. {problem_args.name}\")\n",
    "\n",
    "\n",
    "def get_model(model, width, max_iters, in_channels=3):\n",
    "    model = model.lower()\n",
    "    net = getattr(models, model)(width=width, in_channels=in_channels, max_iters=max_iters)\n",
    "    print(net,'\\n\\n')\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_optimizer(optim_args, model_args, net, state_dict):\n",
    "    optimizer_name = optim_args.optimizer.lower()\n",
    "    epochs = optim_args.epochs\n",
    "    lr = optim_args.lr\n",
    "    lr_decay = optim_args.lr_decay\n",
    "    lr_schedule = optim_args.lr_schedule\n",
    "    lr_factor = optim_args.lr_factor\n",
    "    warmup_period = optim_args.warmup_period\n",
    "\n",
    "    if optim_args.lr_throttle:\n",
    "        # Reducing the lr here for the recurrent layers helps with stability,\n",
    "        # To date (July 21, 2021), we may only need this for maze models.\n",
    "        base_params = [p for n, p in net.named_parameters() if \"recur\" not in n]\n",
    "        recur_params = [p for n, p in net.named_parameters() if \"recur\" in n]\n",
    "        iters = model_args.max_iters\n",
    "        all_params = [{\"params\": base_params}, {\"params\": recur_params, \"lr\": lr / iters}]\n",
    "    else:\n",
    "        base_params = [p for n, p in net.named_parameters()]\n",
    "        recur_params = []\n",
    "        iters = 1\n",
    "        all_params = [{\"params\": base_params}]\n",
    "\n",
    "    wd = 8e-2 # weight decay\n",
    "\n",
    "    if optimizer_name == \"sgd\":\n",
    "        optimizer = SGD(all_params, lr=lr, weight_decay=wd, momentum=0.9)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        optimizer = Adam(all_params, lr=lr, weight_decay=wd)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = AdamW(all_params, lr=lr, weight_decay=wd, betas=(0.9, 0.97))\n",
    "    elif optimizer_name == \"lion\":\n",
    "        optimizer = Lion(all_params, lr=lr, weight_decay=wd, betas=(0.9, 0.99))\n",
    "    elif optimizer_name == \"adam_on_lion\":\n",
    "        optimizer = AdamOnLion(all_params, lr=lr, weight_decay=wd, betas=(0.9, 0.99))\n",
    "    elif optimizer_name == \"adamw_amsgrad\":\n",
    "        optimizer = AdamW(all_params, lr=lr, weight_decay=wd, amsgrad=True)\n",
    "    else:\n",
    "        raise ValueError(f\"{ic.format()}: Optimizer choise of {optimizer_name} not yet implmented.\")\n",
    "\n",
    "    if state_dict is not None:\n",
    "        optimizer.load_state_dict(state_dict)\n",
    "        optimizer.param_groups[0][\"capturable\"] = True # make optimizer capturable=True\n",
    "        warmup_scheduler = ExponentialWarmup(optimizer, warmup_period=0)\n",
    "        # warmup_scheduler = LinearWarmup(optimizer, warmup_period=0)\n",
    "    else:\n",
    "        warmup_scheduler = ExponentialWarmup(optimizer, warmup_period=warmup_period)\n",
    "        # warmup_scheduler = LinearWarmup(optimizer, warmup_period=warmup_period)\n",
    "\n",
    "    if lr_decay.lower() == \"step\":\n",
    "        lr_scheduler = MultiStepLR(optimizer, milestones=lr_schedule,\n",
    "                                   gamma=lr_factor, last_epoch=-1)\n",
    "    elif lr_decay.lower() == \"cosine\":\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, epochs, eta_min=0, last_epoch=-1, verbose=False)\n",
    "    else:\n",
    "        raise ValueError(f\"{ic.format()}: Learning rate decay style {lr_decay} not yet implemented.\")\n",
    "\n",
    "    return optimizer, warmup_scheduler, lr_scheduler\n",
    "\n",
    "\n",
    "def load_model_from_checkpoint(problem, model_args, device, accelerator):\n",
    "    model = model_args.model\n",
    "    model_path = model_args.model_path\n",
    "    width = model_args.width\n",
    "    max_iters = model_args.max_iters\n",
    "    epoch = 0\n",
    "    optimizer = None\n",
    "    new_state_dict = {}\n",
    "\n",
    "    in_channels = 3\n",
    "    if problem == \"chess\":\n",
    "        in_channels = 12\n",
    "    elif problem == 'addition':\n",
    "        in_channels = 1\n",
    "\n",
    "    net = get_model(model, width, in_channels=in_channels, max_iters=max_iters)\n",
    "    net = net.to(device)\n",
    "    if device == \"cuda\":\n",
    "        net = net\n",
    "    \n",
    "    if model_path is not None and os.path.exists(model_path):\n",
    "        logging.info(f\"\\n{'$'*50}\\nLoading model from checkpoint {model_path}...\\n{'$'*50}\")\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "        # check if keys are prefixed with \"module.\"\n",
    "        new_state_dict = state_dict.copy()\n",
    "\n",
    "        for key in list(new_state_dict[\"net\"].keys()):\n",
    "            new_key = key.replace('_orig_mod.', '') # remove _orig_mod. prefix\n",
    "            new_state_dict[\"net\"][new_key] = state_dict['net'][key]\n",
    "            # remove old key\n",
    "            del new_state_dict[\"net\"][key]\n",
    "        \n",
    "        # Now load fixed state_dict\n",
    "        net.load_state_dict(new_state_dict[\"net\"])\n",
    "        epoch = new_state_dict[\"epoch\"] + 1\n",
    "        optimizer = new_state_dict[\"optimizer\"]\n",
    "        accelerator.load_state(f\"/fsx/DPT/outputs/{model_path}\")\n",
    "\n",
    "    return net, epoch, optimizer, accelerator\n",
    "\n",
    "\n",
    "def now():\n",
    "    return datetime.now().strftime(\"%Y%m%d %H:%M:%S\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ddtuzMJpxPVj"
   },
   "source": [
    "## Config `YAML`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z6sYoiHJgHW_"
   },
   "source": [
    "Hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVnF0eHpgCVn",
    "outputId": "e5b24aab-eb40-44f2-970b-28a572059f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/deep-thinking/config/problem/hyp/addition_default.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/config/problem/hyp/addition_default.yaml\n",
    "alpha: 1\n",
    "clip: 1\n",
    "epochs: 250\n",
    "lr: 9e-4 # AdamW -> 9e-4 is decent | Linear scaling rule - New_lr = lr * (new_batch_size / old_bsz)\n",
    "lr_decay: cosine\n",
    "lr_factor: 0.1\n",
    "lr_schedule: #  CosineAnnealingLR Doesn't use this param\n",
    "  - 2\n",
    "lr_throttle: False\n",
    "optimizer: adamw\n",
    "save_period: -1\n",
    "test_batch_size: 640\n",
    "test_mode: default\n",
    "train_batch_size: 640\n",
    "train_mode: progressive\n",
    "val_period: 20\n",
    "warmup_period: 8 # We use Cosine warmup for the first 5 epochs, low warmup for DDP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Look about using difference loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUpaWeQFaEry",
    "outputId": "55bef61e-c269-4777-f3b9-33d8c25d0b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/deep-thinking/config/problem/addition.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./deep-thinking/config/problem/addition.yaml\n",
    "defaults:\n",
    "  - hyp: addition_default\n",
    "  - model: dt_net_recall_1d\n",
    "\n",
    "name: addition\n",
    "test_data: 5_000\n",
    "train_data: 50_000\n",
    "\n",
    "model:\n",
    "  model_path:\n",
    "  width: 512\n",
    "  max_iters: 10\n",
    "  test_iterations:\n",
    "    low: 5\n",
    "    high: 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qwgxRicdd7cp"
   },
   "source": [
    "## Executing the training for Arithmetic ðŸš€"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "10SHnwP5mn3i"
   },
   "source": [
    "Shape -> `(24, 20)`\n",
    "- w denotes the width of the model\n",
    "- Prefix sum used width = 400, so bump 128 â†’ 400\n",
    "- iterations can be pushed to [50,100]\n",
    "- Epochs â†’ 100\n",
    "- Grid search for Î± âˆˆ [0,1]\n",
    "- [x]  Try L1/L2 loss\n",
    "\n",
    "`detnet_1d.py` has been modified\n",
    "`training.py` has been modified\n",
    "`testing.py` has been modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHZTnqLqc7fM",
    "outputId": "774e43f3-c6be-439d-f910-821a178e08ba"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import torch\n",
    "import os\n",
    "\n",
    "%env HYDRA_FULL_ERROR=1\n",
    "%cd /fsx/awesome/DPT/deep-thinking\n",
    "\n",
    "# generate random port b/w 20000-30000\n",
    "port = randrange(20_000, 30_000)\n",
    "rdvz_id = randrange(100, 999)\n",
    "#!python3 train_model.py problem=addition name=addition_run\n",
    "# we have to launch above script with torchrun on a single host, with 8 GPUs\n",
    "%env OMP_NUM_THREADS=2\n",
    "%env PORT=$port\n",
    "%env RDVZ_ID=$rdvz_id\n",
    "#!torchrun --nproc_per_node=1 --nnodes=1 --rdzv_id=$RDVZ_ID --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:$PORT train_model.py problem=addition name=addition_run\n",
    "# Using Huggigface accelerate with DDP and 1 GPU\n",
    "#!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 train_model.py problem=addition name=addition_run\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {n_gpus}\")\n",
    "tpu_available = 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "%env NUM_GPUS=$n_gpus\n",
    "\n",
    "if n_gpus > 0:\n",
    "    !accelerate launch --config_file /fsx/awesome/DPT/configs/acc_config.yaml --num_processes=$NUM_GPUS train_model.py problem=addition name=addition_run\n",
    "elif tpu_available:\n",
    "    print(\"TPU is available\")\n",
    "    !accelerate launch --config_file /fsx/awesome/DPT/configs/acc_tpu_config.yaml train_model.py problem=addition name=addition_run\n",
    "else:\n",
    "    print(\"No GPU or TPU available\")\n",
    "    !accelerate launch --config_file /fsx/awesome/DPT/configs/acc_cpu_config.yaml train_model.py problem=addition name=addition_run\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EV8ezMeXxe8i",
    "kOi3kGPCQvBo",
    "h6UqWoU0bVd-",
    "ddtuzMJpxPVj"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b81162e66a07f2e6ff8b866a7ffebbba4c947ba8c7b07af7b7620290636d340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
